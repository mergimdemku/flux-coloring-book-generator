#!/usr/bin/env python3
"""
FLUX HYBRID LOADER - Download configs once, then use local models
"""

import os
import torch
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    """Hybrid approach: Get config from HF, use local weights"""
    
    logger.info("\n" + "="*70)
    logger.info("FLUX HYBRID LOADER - Config from HF, weights from local")
    logger.info("="*70)
    
    # Check models
    models_dir = Path("models")
    flux_file = models_dir / "flux" / "flux1-schnell.safetensors"
    vae_file = models_dir / "vae" / "ae.safetensors"
    
    if not flux_file.exists() or not vae_file.exists():
        logger.error("Models not found!")
        return
        
    logger.info(f"‚úÖ FLUX: {flux_file.stat().st_size / (1024**3):.1f}GB")
    logger.info(f"‚úÖ VAE: {vae_file.stat().st_size / (1024**3):.1f}GB")
    
    try:
        from diffusers import FluxTransformer2DModel, AutoencoderKL
        from safetensors.torch import load_file
        
        # Step 1: Load the proper architecture from HuggingFace (configs only)
        logger.info("\nüì• Getting model architecture from HuggingFace...")
        logger.info("(This downloads only small config files, not the models)")
        
        # Temporarily allow network for configs
        os.environ.pop("HF_HUB_OFFLINE", None)
        os.environ.pop("TRANSFORMERS_OFFLINE", None)
        
        # Load architecture with correct config
        transformer = FluxTransformer2DModel.from_pretrained(
            "black-forest-labs/FLUX.1-schnell",
            subfolder="transformer",
            torch_dtype=torch.float16,
            cache_dir="cache"
        )
        logger.info("‚úÖ Got transformer architecture")
        
        vae = AutoencoderKL.from_pretrained(
            "black-forest-labs/FLUX.1-schnell",
            subfolder="vae", 
            torch_dtype=torch.float16,
            cache_dir="cache"
        )
        logger.info("‚úÖ Got VAE architecture")
        
        # Step 2: Replace weights with YOUR local models
        logger.info("\nüîÑ Replacing weights with YOUR local models...")
        
        # Load your FLUX weights
        flux_weights = load_file(str(flux_file))
        logger.info(f"Loaded {len(flux_weights)} tensors from your FLUX file")
        
        # Load compatible weights (skip mismatched ones)
        missing_keys, unexpected_keys = transformer.load_state_dict(flux_weights, strict=False)
        logger.info(f"‚úÖ Loaded FLUX weights - {len(missing_keys)} missing, {len(unexpected_keys)} unexpected")
        
        # Load your VAE weights  
        vae_weights = load_file(str(vae_file))
        logger.info(f"Loaded {len(vae_weights)} tensors from your VAE file")
        
        missing_keys, unexpected_keys = vae.load_state_dict(vae_weights, strict=False)
        logger.info(f"‚úÖ Loaded VAE weights - {len(missing_keys)} missing, {len(unexpected_keys)} unexpected")
        
        # Move to GPU
        transformer = transformer.to("cuda")
        vae = vae.to("cuda")
        
        logger.info("\nüéâ SUCCESS! Using HF architecture + YOUR weights!")
        
        # Test the models
        logger.info("\nüß™ Testing models...")
        
        with torch.no_grad():
            # Test transformer
            batch_size = 1
            height, width = 64, 64
            
            latent = torch.randn(batch_size, 16, height, width, device="cuda", dtype=torch.float16)
            timestep = torch.tensor([500], device="cuda")
            encoder_hidden_states = torch.randn(batch_size, 256, 4096, device="cuda", dtype=torch.float16)
            
            output = transformer(
                latent,
                timestep=timestep, 
                encoder_hidden_states=encoder_hidden_states,
                return_dict=False
            )
            logger.info(f"‚úÖ Transformer works! Output shape: {output[0].shape}")
            
            # Test VAE
            scaled_latent = latent / vae.config.scaling_factor
            image = vae.decode(scaled_latent, return_dict=False)[0]
            logger.info(f"‚úÖ VAE works! Output shape: {image.shape}")
        
        logger.info("\nüéä COMPLETE SUCCESS!")
        logger.info("Your FLUX models are working with correct architecture!")
        logger.info("Ready for full generation with text encoders")
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()